{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Observations\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    " \r\n",
    "def mean_squared_error(y, X = []):\r\n",
    "    return sum((y-X)**2)/len(X)\r\n",
    "\r\n",
    "# def mean_squared_error(y, X = []):\r\n",
    "#     return ((y - X) ** 2).mean()\r\n",
    "coeff = 12/16\r\n",
    "def gradient_descent(_X, _y, state=0 ,_learningrate = 0, _epochs=5):\r\n",
    "    # trace = pd.DataFrame(columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'])\r\n",
    "    trace = pd.DataFrame(columns=['A', 'B', 'C', 'D', 'my_delta'])\r\n",
    "    x = np.array(_X)\r\n",
    "    y = np.array(_y)\r\n",
    "    # a, b = 0.0, 0.0 # Initialisation aléatoire de a et b\r\n",
    "    # mse = []\r\n",
    "    N = len(_X) \r\n",
    "    # temp_mse = mean_squared_error(y, (a*x + b))\r\n",
    "    # temp_learningrate = _learningrate   \r\n",
    "    # A,B,C,D = 1.0, 1.0, 0.0, 0.0    \r\n",
    "    A = beta[0]*coeff\r\n",
    "    B = beta[1]*coeff\r\n",
    "    # C = sum(x)/len(x) + np.random.randn()\r\n",
    "    # D = sum(y)/len(y) + np.random.randn()\r\n",
    "    C = beta[2]\r\n",
    "    D = beta[3]\r\n",
    "    temp_A = A\r\n",
    "    temp_B = B\r\n",
    "                                   \r\n",
    "    equation = lambda A,B,C,D,X,Y: A*X.dot(X).sum()+B*Y.dot(Y).sum()+C*X.sum()+D*Y.sum()\r\n",
    "\r\n",
    "    equation1 = lambda A,B,C,D,X,Y: ((X-C)*(X-C))/(A*A)+( ((Y-D)*(Y-D)) /(B*B))\r\n",
    "    equation2 = lambda A,B,C,D,X,Y: ((X*(1-1/((X/A)**2+(Y/B)**2)**(1/2)))**2+(Y*(1-1/((X/A)**2+(Y/B)**2)**(1/2)))**2)**(1/2)\r\n",
    "    equation3 = lambda A,B,C,D,X,Y: ((X-( C+(X-C)/( ( (((X-C)**2)/(A**2))+(((Y-D)**2)/(B**2)) )**(1/2) )) )**2+ (Y-( D+(Y-D)/( ( (((X-C)**2)/(A**2))+(((Y-D)**2)/(B**2)) )**(1/2) )) )**2)**(1/2)\r\n",
    "    \r\n",
    "    # derive1 = lambda A,B,C,D,X,Y: 2*A*X.dot(X.dot(X.dot(X))).sum()+2*B*X.dot(X.dot(Y.dot(Y))).sum()+2*C*X.dot(X.dot(X)).sum()+2*D*X.dot(X.dot(Y)).sum()-2*X.dot(X).sum()\r\n",
    "    # derive1(A,B,C,D,x,y) = derive en fonction de A\r\n",
    "    # derive1(B,A,D,C,y,x) = derive en fonction de B\r\n",
    "    acc1 = lambda X : 2*X.dot(X.dot(X.dot(X))).sum()\r\n",
    "    # acc1(x)\r\n",
    "    # acc1(y)\r\n",
    "    derive2= lambda A,B,C,D,X,Y: 2*C*X.dot(X).sum()+2*A*X.dot(X.dot(X)).sum()+2*B*Y.dot(Y.dot(X)).sum()+2*D*X.dot(Y).sum()-2*X.sum()\r\n",
    "    # derive2(A,B,C,D,x,y) = derive en fonction de C\r\n",
    "    # derive2(B,A,D,C,y,x) = derive en fonction de D\r\n",
    "    acc2 = lambda X : 2*X.dot(X).sum()\r\n",
    "    # acc2(x)\r\n",
    "    # acc2(y)\r\n",
    "    my_delta = np.array(equation3(A,B,C,D,x,y))\r\n",
    "    \r\n",
    "    dA = ( (-2/N)* (1) * x.dot(x.dot(my_delta)).sum() )\r\n",
    "    dB = ( (-2/N)* (1) * y.dot(y.dot(my_delta)).sum() )\r\n",
    "    dC = (-2/N)* x.dot(my_delta).sum()\r\n",
    "    dD = (-2/N)* y.dot(my_delta).sum()\r\n",
    "\r\n",
    "    # mse = mean_squared_error(y, (a*x + b))\r\n",
    "    # trace = trace.append(pd.DataFrame(data=[[A, dA, B, dB, C, dC, D, dD, my_delta.sum()]], \r\n",
    "    #                                   columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'], \r\n",
    "    #                                   index=['epoch ' + str(0)]))\r\n",
    "                                      \r\n",
    "        \r\n",
    "    trace = trace.append(pd.DataFrame(data=[[A, B, C, D, my_delta.sum()]], \r\n",
    "                                      columns=['A', 'B', 'C', 'D', 'my_delta'], \r\n",
    "                                      index=['epoch ' + str(0)]))\r\n",
    "                                      \r\n",
    "    print(\"\\nbegin: x\\n\", x, \"\\nend: x\\n\",\"\\nbegin: y\\n\", y, \"\\nend: y\\n\")\r\n",
    "\r\n",
    "    lowest = [100000,0,0,0,0]\r\n",
    "    for i in range(_epochs):\r\n",
    "        # delta = y - (a*x + b)\r\n",
    "        my_delta = np.array(equation3(A,B,C,D,x,y))\r\n",
    "        # print(\"\\nbegin: state\\n\", i,\"|\",A,\"|\",B,\"|\",C,\"|\",D,\"|\",my_delta.sum(),\"\\n\", ellipse,\"\\nend: state\\n\",\"\\nbegin: my_delta\\n\", my_delta, \"\\nend: my_delta\\n\")\r\n",
    "        # for j in my_delta:\r\n",
    "        #   print(j) \r\n",
    "        if (abs(my_delta.sum()) >= abs(lowest[0])) and i >= 10 : break\r\n",
    "        if abs(my_delta.sum()) <= abs(lowest[0]): lowest=[my_delta.sum(),A,B,C,D]\r\n",
    "        # delta_square = (y - (a*X + b))**2\r\n",
    "\r\n",
    "        # temp_learningrate = _learningrate/((i+1)**0.25)\r\n",
    "        # temp2a_learningrate = _learningrate*(2*sum(X**2))/N\r\n",
    "        # temp2b_learningrate = _learningrate*(2)/N\r\n",
    "        # temp3a_learningrate = _learningrate/((N**2-2*sum(X**2))**(1/2))\r\n",
    "        # temp3b_learningrate = _learningrate/((N**2-2)**(1/2))\r\n",
    "        # print(_learningrate,temp2a_learningrate, temp2b_learningrate, temp3a_learningrate, temp3b_learningrate, temp_learningrate)\r\n",
    "        # Updating a and b\r\n",
    "        # print(delta,delta_square,X,X.dot(delta).sum(), delta.sum(), N)\r\n",
    "        if state == 1:\r\n",
    "            # a = a -temp3a_learningrate * (-2 * x.dot(delta).sum()) # on retire un gradient à a\r\n",
    "            # b = b -temp3b_learningrate * (-2 * delta.sum()) # idem pour b\r\n",
    "            # print(\"lol\")\r\n",
    "            \r\n",
    "            # A_temp = (A/abs(A))*max(abs(A), 0.01)\r\n",
    "            # B_temp = (B/abs(B))*max(abs(B), 0.01)\r\n",
    "            A_temp = A\r\n",
    "            B_temp = B\r\n",
    "            C_temp = C\r\n",
    "            D_temp = D\r\n",
    "            # dA = (1/(A_temp*A_temp*A_temp)) * (x-C_temp).dot(x-C_temp).sum() \r\n",
    "            # dB = (1/(B_temp*B_temp*B_temp)) * (y-D_temp).dot(y-D_temp).sum() \r\n",
    "            # dC = ((10) * (C_temp-x).sum()/(A_temp*A_temp))**2\r\n",
    "            # dD = ((10) * (D_temp-y).sum()/(B_temp*B_temp))**2 \r\n",
    "\r\n",
    "\r\n",
    "            # if abs(dA) >= abs(dB) and abs(dA) >= abs(dC) and abs(dA) >= abs(dD):\r\n",
    "            #   A = A -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(A_temp*A_temp*A_temp)) * (x-C_temp).dot(x-C_temp).sum() )\r\n",
    "            # if abs(dB) >= abs(dA) and abs(dB) >= abs(dC) and abs(dB) >= abs(dD):\r\n",
    "            #   B = B -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(B_temp*B_temp*B_temp)) * (y-D_temp).dot(y-D_temp).sum() )\r\n",
    "            # if abs(dC) >= abs(dA) and abs(dC) >= abs(dB) and abs(dC) >= abs(dD):\r\n",
    "            #   C = C -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (10) * (C_temp-x).sum()/(A_temp*A_temp) )\r\n",
    "            # if abs(dD) >= abs(dA) and abs(dD) >= abs(dB) and abs(dD) >= abs(dC):\r\n",
    "            #   D = D -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (10) * (D_temp-y).sum()/(B_temp*B_temp) )\r\n",
    "            \r\n",
    "            # A = A -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(A_temp*A_temp*A_temp)) * ((x-C_temp)**2).dot(my_delta).sum() )\r\n",
    "            # B = B -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(B_temp*B_temp*B_temp)) * ((y-D_temp)**2).dot(my_delta).sum() )\r\n",
    "            # C = C -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (2/(A_temp*A_temp)) * ((C_temp-x.dot(my_delta))).sum() )\r\n",
    "            # D = D -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (2/(B_temp*B_temp)) * ((D_temp-y.dot(my_delta))).sum() )\r\n",
    "            \r\n",
    "            A = A -_learningrate * ( (-2*(1/N))* (1/(A_temp*A_temp*A_temp)) * (abs((x-C_temp)**2)).dot(my_delta).sum() )\r\n",
    "            B = B -_learningrate * ( (-2*(1/N))* (1/(B_temp*B_temp*B_temp)) * (abs((y-D_temp)**2)).dot(my_delta).sum() )\r\n",
    "            C = C -_learningrate * ( (-2*(1/(N*N)))* (1/(A_temp*A_temp)) * (((C_temp-x).dot(my_delta)).sum()) )\r\n",
    "            D = D -_learningrate * ( (-2*(1/(N*N)))* (1/(B_temp*B_temp)) * (((D_temp-y).dot(my_delta)).sum()) )\r\n",
    "            \r\n",
    "            # A = A -_learningrate * ( (-2/N)* (1) * x.dot(x.dot(my_delta)).sum() )\r\n",
    "            # B = B -_learningrate * ( (-2/N)* (1) * y.dot(y.dot(my_delta)).sum() )\r\n",
    "            # A = A -const*_learningrate * ( (-2/N)* x.dot(x.dot(my_delta)).sum() )\r\n",
    "            # B = B -const*_learningrate * ( (-2/N)* y.dot(y.dot(my_delta)).sum() )\r\n",
    "            # C = C -const*_learningrate * ( (-2/N)* x.dot(my_delta).sum() )\r\n",
    "            # D = D -const*_learningrate * ( (-2/N)* y.dot(my_delta).sum() )\r\n",
    "            # print(\"new\",A,B,C,D,x,y)\r\n",
    "        else:\r\n",
    "            if state == 2:\r\n",
    "                # a = a -_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                # b = b -_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "\r\n",
    "                A_temp = A\r\n",
    "                B_temp = B\r\n",
    "                C_temp = C\r\n",
    "                D_temp = D\r\n",
    "                dA = derive1(A_temp, B_temp, C_temp, D_temp, x, y)\r\n",
    "                dB = derive1(B_temp, A_temp, D_temp, C_temp, y, x)\r\n",
    "                dC = derive2(A_temp, B_temp, C_temp, D_temp, x, y)\r\n",
    "                dD = derive2(B_temp, A_temp, D_temp, C_temp, y, x)\r\n",
    "\r\n",
    "                A = A +const*_learningrate * dA / N\r\n",
    "                B = B +const*_learningrate * dB / N\r\n",
    "                C = C +const*_learningrate * dC / N\r\n",
    "                D = D +const*_learningrate * dD / N\r\n",
    "                # print(\"new\",A,B,C,D,x,y)\r\n",
    "            # else:\r\n",
    "                # if state == 3:\r\n",
    "                #     a = a -temp2a_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                #     b = b -temp2b_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "                # else:\r\n",
    "                #     if state ==4:\r\n",
    "                #         a = a -temp_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                #         b = b -temp_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "                #     else:\r\n",
    "                #         if state == 5:\r\n",
    "                #             temp = x.copy()\r\n",
    "                #             temp2 =x.dot(delta).sum()\r\n",
    "                #             temp3 =x.dot(temp).sum()\r\n",
    "                #             a = a -_learningrate * ((-2 * temp2 / N)*(1 - (-2 * temp3 /N)) ) # on retire un gradient à a\r\n",
    "                #             b = b -_learningrate * ((-2 * delta.sum() / N )*(1 - (2/N)) ) # idem pour b\r\n",
    "                #         else:\r\n",
    "                #             if state == 6:\r\n",
    "                #                 a = a -_learningrate * ((-2 * x.dot(delta).sum() / N)*(1 + (2 * x.dot(x.copy()).sum() /N)) ) # on retire un gradient à a\r\n",
    "                #                 b = b -_learningrate * ((-2 * delta.sum() / N )*(1 + (2/N)) ) # idem pour b\r\n",
    "        # a = a -_learningrate * (-2 * X.dot(delta).sum()) # on retire un gradient à a\r\n",
    "        # b = b -_learningrate * (-2 * delta.sum()) # idem pour b\r\n",
    "        # print(y, (a*X + b),a,X,b)\r\n",
    "        # mse = mean_squared_error(y, (a*x + b))\r\n",
    "        # mse2 = mean_squared_error(y, ( ( (1-A**2*(x-B)**2)/(C**2) )**(1/2) + D ) )\r\n",
    "        # mse2 = mean_squared_error(y, (A*x**2) )\r\n",
    "        # print(mse2)\r\n",
    " \r\n",
    "        # trace = trace.append(pd.DataFrame(data=[[A, dA, B, dB, C, dC, D, dD, my_delta.sum()]], \r\n",
    "        #                                   columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'], \r\n",
    "        #                                   index=['epoch ' + str(i+1)]))\r\n",
    "                                          \r\n",
    "        trace = trace.append(pd.DataFrame(data=[[A, B, C, D, my_delta.sum()]], \r\n",
    "                                          columns=['A', 'B', 'C', 'D', 'my_delta'], \r\n",
    "                                          index=['epoch ' + str(i+1)]))\r\n",
    "        # print(i+1, a, temp_a,temp_a==a, b, temp_b,temp_b==b, mse, temp_mse, temp_mse==mse)\r\n",
    "        # print(i+1)\r\n",
    "        # if temp_A == A and temp_B == B:\r\n",
    "        #     # print(i)\r\n",
    "        #     break\r\n",
    "        # temp_A = A\r\n",
    "        # temp_B = B\r\n",
    "        # temp_mse = mse\r\n",
    "    print(lowest)\r\n",
    "    return A, B, C, D, my_delta.sum(), trace\r\n",
    "# 2320 1.0999999999999988 1.0999999999999988 True  -0.0666666666666641  -0.06666666666666408 False 0.05555555555555553  0.055555555555555504 False\r\n",
    "# 2321 1.0999999999999988 1.0999999999999988 True  -0.0666666666666641  -0.0666666666666641  True  0.05555555555555553  0.05555555555555553  True\r\n",
    "\r\n",
    "# 9999 1.1000000000000514 1.0999999999999488 False -0.06666666666664432 -0.06666666666668944 False 0.055555555555555504 0.05555555555555556  False\r\n",
    "#10000 1.0999999999999488 1.1000000000000514 False -0.06666666666668944 -0.06666666666664432 False 0.05555555555555556  0.055555555555555504 False\r\n",
    "\r\n",
    "# 9999 1.0999999984358646 1.0999999984336053 False -0.06666666311102022 -0.06666666310588418 False 0.055555555555555504 0.05555555555555556  False\r\n",
    "#10000 1.0999999984381206 1.0999999984358646 False -0.06666666311614873 -0.06666666311102022 False 0.05555555555555553  0.055555555555555504 False\r\n",
    "def displayResult(_A, _B, _C, _D, _my_delta, _trace):\r\n",
    "  plt.figure( figsize=(30,5))\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 1)\r\n",
    "  plt.grid(True)\r\n",
    "  plt.title(\"points\")\r\n",
    "  plt.xlabel(\"x, A,C\")\r\n",
    "  plt.ylabel(\"y, B,D\")\r\n",
    "  plt.scatter(Xinit,Yinit)\r\n",
    "  from math import pi, cos, sin\r\n",
    "  t = np.linspace(0, 2*pi, 100)\r\n",
    "  Ell = np.array([_A*np.cos(t) , _B*np.sin(t)])  \r\n",
    "  plt.plot( _C+Ell[0,:] , _D+Ell[1,:], 'g' )     #initial ellipse\r\n",
    "\r\n",
    "  t = np.linspace(0, 2*pi, 100)\r\n",
    "  Ell = np.array([beta[0]*coeff*np.cos(t) , beta[1]*coeff*np.sin(t)])  \r\n",
    "  plt.plot( beta[2]+Ell[0,:] , beta[3]+Ell[1,:], 'r')     #initial ellipse\r\n",
    "#   plt.plot([X[0], X[len(X)-1]], [_a * X[0] + _b, _a * X[len(X)-1] + _b], 'r-', lw=2)\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 2)\r\n",
    "  plt.title(\"A\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['A'])\r\n",
    "  \r\n",
    "#   plt.subplot(1, 9, 3)\r\n",
    "#   plt.title(\"dA\")\r\n",
    "#   plt.plot(_trace['dA'])\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 3)\r\n",
    "  plt.title(\"B\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['B'])\r\n",
    " \r\n",
    "#   plt.subplot(1, 9, 5)\r\n",
    "#   plt.title(\"dB\")\r\n",
    "#   plt.plot(_trace['dB'])\r\n",
    "  \r\n",
    "  plt.subplot(1, 6, 4)\r\n",
    "  plt.title(\"C\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['C'])\r\n",
    "\r\n",
    "#   plt.subplot(1, 9, 7)\r\n",
    "#   plt.title(\"dC\")\r\n",
    "#   plt.plot(_trace['dC'])\r\n",
    "\r\n",
    "  plt.subplot(1, 6, 5)\r\n",
    "  plt.title(\"D\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['D'])\r\n",
    "  \r\n",
    "#   plt.subplot(1, 9, 9)\r\n",
    "#   plt.title(\"dD\")\r\n",
    "#   plt.plot(_trace['dD'])\r\n",
    "\r\n",
    "  plt.subplot(1, 6, 6)\r\n",
    "  plt.title(\"my_delta\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['my_delta'])\r\n",
    "  \r\n",
    "  # plt.subplot(1, 5, 5)\r\n",
    "  print (_trace)\r\n",
    "  print(ellipse)\r\n",
    "  plt.show()\r\n",
    "\r\n",
    "# X = [1.0, 2.0, 3.0]\r\n",
    "# y = [1.2, 1.8, 3.4]\r\n",
    "# print(A,B)\r\n",
    "# X = []\r\n",
    "# y = []\r\n",
    "# for i in range(0,1000):\r\n",
    "#     to_x =np.random(0,1)\r\n",
    "#     X.append(to_x)\r\n",
    "#     y.append(A*to_x + B)\r\n",
    "# np.random.seed(1727773457)\r\n",
    "# np.random.seed(72172712)\r\n",
    "\r\n",
    "Ainit,Binit = 1.1, -2/3\r\n",
    "\r\n",
    "num_step = 10000\r\n",
    "num_var = 100\r\n",
    "alpha = [-10, -10, 10, -10]\r\n",
    "beta = [100,100,1000,-1000]\r\n",
    "randi = 1\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "# samples2 = np.random.randn(num_var)\r\n",
    "# X = samples + ellipse[0]\r\n",
    "# Y = ( ( (1-ellipse[2]**2*(X-ellipse[1])**2)/(ellipse[3]**2) )**(1/2) + ellipse[1] ) \r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "# X = ellipse[2]*np.cos(2*samples1) \r\n",
    "# Y = ellipse[3]*np.sin(2*samples1) \r\n",
    "\r\n",
    "# print(X,Y)\r\n",
    "# X1 = samples\r\n",
    "# Y1 = Ainit*samples**2 + Binit *  + np.random.randn(num_var)\r\n",
    "\r\n",
    "def do_show(Aa,Ba,Ca=1,D=1000, learningrate = 0.05):\r\n",
    "    # print(A,B)\r\n",
    "    A, B, C, D, my_delta, trace = gradient_descent(Aa, Ba, Ca, learningrate, _epochs=D)\r\n",
    "    displayResult(A, B, C, D, my_delta, trace )\r\n",
    "\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step) \r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "# do_show(X.copy(),Y.copy(),2,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),1,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),5,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),6,num_step)\r\n",
    "# a, b, trace = gradient_descent(X, y, 3, _epochs=100000)\r\n",
    "# displayResult(a, b, trace)\r\n",
    "# a, b, trace = gradient_descent(X, y, 4, _epochs=100000)\r\n",
    "# displayResult(a, b, trace)\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "\r\n",
    "# 0.951333380260973\r\n",
    "# 0.9513333802609729\r\n",
    "# 0.9513349597305195\r\n",
    "# 0.951333380260973\r\n",
    "\r\n",
    "# 285 1.1305812000178983 1.130581200017898 False -0.6648922306427782 -0.6648922306427781 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014590149504239829\r\n",
    "# 286 1.1305812000178983 1.1305812000178983 True -0.6648922306427784 -0.6648922306427782 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014577423674859143\r\n",
    "# 287 1.1305812000178983 1.1305812000178983 True -0.6648922306427785 -0.6648922306427784 False 0.9513333802609731 0.951333380260973 False\r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014564753151219703\r\n",
    "# 288 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427785 False 0.951333380260973 0.9513333802609731 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014552137502179978\r\n",
    "# 289 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427786 True 0.951333380260973 0.951333380260973 True        \r\n",
    "\r\n",
    "# 285 1.1305812000178983 1.130581200017898 False -0.6648922306427782 -0.6648922306427781 False 0.951333380260973 0.951333380260973 True\r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014590149504239829\r\n",
    "# 286 1.1305812000178983 1.1305812000178983 True -0.6648922306427784 -0.6648922306427782 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014577423674859143\r\n",
    "# 287 1.1305812000178983 1.1305812000178983 True -0.6648922306427785 -0.6648922306427784 False 0.9513333802609731 0.951333380260973 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014564753151219703\r\n",
    "# 288 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427785 False 0.951333380260973 0.9513333802609731 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014552137502179978\r\n",
    "# 289 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427786 True 0.951333380260973 0.951333380260973 True        \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 Observations\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    " \r\n",
    "def mean_squared_error(y, X = []):\r\n",
    "    return sum((y-X)**2)/len(X)\r\n",
    "\r\n",
    "# def mean_squared_error(y, X = []):\r\n",
    "#     return ((y - X) ** 2).mean()\r\n",
    "coeff = 12/16\r\n",
    "def gradient_descent(_X, _y, state=0 ,_learningrate = 0, _epochs=5):\r\n",
    "    # trace = pd.DataFrame(columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'])\r\n",
    "    trace = pd.DataFrame(columns=['A', 'B', 'C', 'D', 'my_delta'])\r\n",
    "    x = np.array(_X)\r\n",
    "    y = np.array(_y)\r\n",
    "    # a, b = 0.0, 0.0 # Initialisation aléatoire de a et b\r\n",
    "    # mse = []\r\n",
    "    N = len(_X) \r\n",
    "    # temp_mse = mean_squared_error(y, (a*x + b))\r\n",
    "    # temp_learningrate = _learningrate   \r\n",
    "    # A,B,C,D = 1.0, 1.0, 0.0, 0.0    \r\n",
    "    A = beta[0]*coeff\r\n",
    "    B = beta[1]*coeff\r\n",
    "    # C = sum(x)/len(x) + np.random.randn()\r\n",
    "    # D = sum(y)/len(y) + np.random.randn()\r\n",
    "    C = beta[2]\r\n",
    "    D = beta[3]\r\n",
    "    temp_A = A\r\n",
    "    temp_B = B\r\n",
    "                                   \r\n",
    "    equation = lambda A,B,C,D,X,Y: A*X.dot(X).sum()+B*Y.dot(Y).sum()+C*X.sum()+D*Y.sum()\r\n",
    "\r\n",
    "    equation1 = lambda A,B,C,D,X,Y: ((X-C)*(X-C))/(A*A)+( ((Y-D)*(Y-D)) /(B*B))\r\n",
    "    equation2 = lambda A,B,C,D,X,Y: ((X*(1-1/((X/A)**2+(Y/B)**2)**(1/2)))**2+(Y*(1-1/((X/A)**2+(Y/B)**2)**(1/2)))**2)**(1/2)\r\n",
    "    equation3 = lambda A,B,C,D,X,Y: ((X-( C+(X-C)/( ( (((X-C)**2)/(A**2))+(((Y-D)**2)/(B**2)) )**(1/2) )) )**2+ (Y-( D+(Y-D)/( ( (((X-C)**2)/(A**2))+(((Y-D)**2)/(B**2)) )**(1/2) )) )**2)**(1/2)\r\n",
    "    \r\n",
    "    # derive1 = lambda A,B,C,D,X,Y: 2*A*X.dot(X.dot(X.dot(X))).sum()+2*B*X.dot(X.dot(Y.dot(Y))).sum()+2*C*X.dot(X.dot(X)).sum()+2*D*X.dot(X.dot(Y)).sum()-2*X.dot(X).sum()\r\n",
    "    # derive1(A,B,C,D,x,y) = derive en fonction de A\r\n",
    "    # derive1(B,A,D,C,y,x) = derive en fonction de B\r\n",
    "    acc1 = lambda X : 2*X.dot(X.dot(X.dot(X))).sum()\r\n",
    "    # acc1(x)\r\n",
    "    # acc1(y)\r\n",
    "    derive2= lambda A,B,C,D,X,Y: 2*C*X.dot(X).sum()+2*A*X.dot(X.dot(X)).sum()+2*B*Y.dot(Y.dot(X)).sum()+2*D*X.dot(Y).sum()-2*X.sum()\r\n",
    "    # derive2(A,B,C,D,x,y) = derive en fonction de C\r\n",
    "    # derive2(B,A,D,C,y,x) = derive en fonction de D\r\n",
    "    acc2 = lambda X : 2*X.dot(X).sum()\r\n",
    "    # acc2(x)\r\n",
    "    # acc2(y)\r\n",
    "    my_delta = np.array(equation3(A,B,C,D,x,y))\r\n",
    "    \r\n",
    "    dA = ( (-2/N)* (1) * x.dot(x.dot(my_delta)).sum() )\r\n",
    "    dB = ( (-2/N)* (1) * y.dot(y.dot(my_delta)).sum() )\r\n",
    "    dC = (-2/N)* x.dot(my_delta).sum()\r\n",
    "    dD = (-2/N)* y.dot(my_delta).sum()\r\n",
    "\r\n",
    "    # mse = mean_squared_error(y, (a*x + b))\r\n",
    "    # trace = trace.append(pd.DataFrame(data=[[A, dA, B, dB, C, dC, D, dD, my_delta.sum()]], \r\n",
    "    #                                   columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'], \r\n",
    "    #                                   index=['epoch ' + str(0)]))\r\n",
    "                                      \r\n",
    "        \r\n",
    "    trace = trace.append(pd.DataFrame(data=[[A, B, C, D, my_delta.sum()]], \r\n",
    "                                      columns=['A', 'B', 'C', 'D', 'my_delta'], \r\n",
    "                                      index=['epoch ' + str(0)]))\r\n",
    "                                      \r\n",
    "    print(\"\\nbegin: x\\n\", x, \"\\nend: x\\n\",\"\\nbegin: y\\n\", y, \"\\nend: y\\n\")\r\n",
    "\r\n",
    "    lowest = [100000,0,0,0,0]\r\n",
    "    for i in range(_epochs):\r\n",
    "        # delta = y - (a*x + b)\r\n",
    "        my_delta = np.array(equation3(A,B,C,D,x,y))\r\n",
    "        # print(\"\\nbegin: state\\n\", i,\"|\",A,\"|\",B,\"|\",C,\"|\",D,\"|\",my_delta.sum(),\"\\n\", ellipse,\"\\nend: state\\n\",\"\\nbegin: my_delta\\n\", my_delta, \"\\nend: my_delta\\n\")\r\n",
    "        # for j in my_delta:\r\n",
    "        #   print(j) \r\n",
    "        if (abs(my_delta.sum()) >= abs(lowest[0])) and i >= 10 : break\r\n",
    "        if abs(my_delta.sum()) <= abs(lowest[0]): lowest=[my_delta.sum(),A,B,C,D]\r\n",
    "        # delta_square = (y - (a*X + b))**2\r\n",
    "\r\n",
    "        # temp_learningrate = _learningrate/((i+1)**0.25)\r\n",
    "        # temp2a_learningrate = _learningrate*(2*sum(X**2))/N\r\n",
    "        # temp2b_learningrate = _learningrate*(2)/N\r\n",
    "        # temp3a_learningrate = _learningrate/((N**2-2*sum(X**2))**(1/2))\r\n",
    "        # temp3b_learningrate = _learningrate/((N**2-2)**(1/2))\r\n",
    "        # print(_learningrate,temp2a_learningrate, temp2b_learningrate, temp3a_learningrate, temp3b_learningrate, temp_learningrate)\r\n",
    "        # Updating a and b\r\n",
    "        # print(delta,delta_square,X,X.dot(delta).sum(), delta.sum(), N)\r\n",
    "        if state == 1:\r\n",
    "            # a = a -temp3a_learningrate * (-2 * x.dot(delta).sum()) # on retire un gradient à a\r\n",
    "            # b = b -temp3b_learningrate * (-2 * delta.sum()) # idem pour b\r\n",
    "            # print(\"lol\")\r\n",
    "            \r\n",
    "            # A_temp = (A/abs(A))*max(abs(A), 0.01)\r\n",
    "            # B_temp = (B/abs(B))*max(abs(B), 0.01)\r\n",
    "            A_temp = A\r\n",
    "            B_temp = B\r\n",
    "            C_temp = C\r\n",
    "            D_temp = D\r\n",
    "            # dA = (1/(A_temp*A_temp*A_temp)) * (x-C_temp).dot(x-C_temp).sum() \r\n",
    "            # dB = (1/(B_temp*B_temp*B_temp)) * (y-D_temp).dot(y-D_temp).sum() \r\n",
    "            # dC = ((10) * (C_temp-x).sum()/(A_temp*A_temp))**2\r\n",
    "            # dD = ((10) * (D_temp-y).sum()/(B_temp*B_temp))**2 \r\n",
    "\r\n",
    "\r\n",
    "            # if abs(dA) >= abs(dB) and abs(dA) >= abs(dC) and abs(dA) >= abs(dD):\r\n",
    "            #   A = A -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(A_temp*A_temp*A_temp)) * (x-C_temp).dot(x-C_temp).sum() )\r\n",
    "            # if abs(dB) >= abs(dA) and abs(dB) >= abs(dC) and abs(dB) >= abs(dD):\r\n",
    "            #   B = B -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(B_temp*B_temp*B_temp)) * (y-D_temp).dot(y-D_temp).sum() )\r\n",
    "            # if abs(dC) >= abs(dA) and abs(dC) >= abs(dB) and abs(dC) >= abs(dD):\r\n",
    "            #   C = C -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (10) * (C_temp-x).sum()/(A_temp*A_temp) )\r\n",
    "            # if abs(dD) >= abs(dA) and abs(dD) >= abs(dB) and abs(dD) >= abs(dC):\r\n",
    "            #   D = D -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (10) * (D_temp-y).sum()/(B_temp*B_temp) )\r\n",
    "            \r\n",
    "            # A = A -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(A_temp*A_temp*A_temp)) * ((x-C_temp)**2).dot(my_delta).sum() )\r\n",
    "            # B = B -_learningrate * ( (-2*(1/N-2/(N*N)+1/(N*N*N) ))* (1/(B_temp*B_temp*B_temp)) * ((y-D_temp)**2).dot(my_delta).sum() )\r\n",
    "            # C = C -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (2/(A_temp*A_temp)) * ((C_temp-x.dot(my_delta))).sum() )\r\n",
    "            # D = D -_learningrate * ( (2*(1/N-2/(N*N)+1/(N*N*N) ))* (2/(B_temp*B_temp)) * ((D_temp-y.dot(my_delta))).sum() )\r\n",
    "            \r\n",
    "            A = A -_learningrate * ( (-2*(1/N))* (1/(A_temp*A_temp*A_temp)) * (abs((x-C_temp)**2)).dot(my_delta).sum() )\r\n",
    "            B = B -_learningrate * ( (-2*(1/N))* (1/(B_temp*B_temp*B_temp)) * (abs((y-D_temp)**2)).dot(my_delta).sum() )\r\n",
    "            C = C -_learningrate * ( (-2*(1/(N*N)))* (1/(A_temp*A_temp)) * (((C_temp-x).dot(my_delta)).sum()) )\r\n",
    "            D = D -_learningrate * ( (-2*(1/(N*N)))* (1/(B_temp*B_temp)) * (((D_temp-y).dot(my_delta)).sum()) )\r\n",
    "            \r\n",
    "            # A = A -_learningrate * ( (-2/N)* (1) * x.dot(x.dot(my_delta)).sum() )\r\n",
    "            # B = B -_learningrate * ( (-2/N)* (1) * y.dot(y.dot(my_delta)).sum() )\r\n",
    "            # A = A -const*_learningrate * ( (-2/N)* x.dot(x.dot(my_delta)).sum() )\r\n",
    "            # B = B -const*_learningrate * ( (-2/N)* y.dot(y.dot(my_delta)).sum() )\r\n",
    "            # C = C -const*_learningrate * ( (-2/N)* x.dot(my_delta).sum() )\r\n",
    "            # D = D -const*_learningrate * ( (-2/N)* y.dot(my_delta).sum() )\r\n",
    "            # print(\"new\",A,B,C,D,x,y)\r\n",
    "        else:\r\n",
    "            if state == 2:\r\n",
    "                # a = a -_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                # b = b -_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "\r\n",
    "                A_temp = A\r\n",
    "                B_temp = B\r\n",
    "                C_temp = C\r\n",
    "                D_temp = D\r\n",
    "                dA = derive1(A_temp, B_temp, C_temp, D_temp, x, y)\r\n",
    "                dB = derive1(B_temp, A_temp, D_temp, C_temp, y, x)\r\n",
    "                dC = derive2(A_temp, B_temp, C_temp, D_temp, x, y)\r\n",
    "                dD = derive2(B_temp, A_temp, D_temp, C_temp, y, x)\r\n",
    "\r\n",
    "                A = A +const*_learningrate * dA / N\r\n",
    "                B = B +const*_learningrate * dB / N\r\n",
    "                C = C +const*_learningrate * dC / N\r\n",
    "                D = D +const*_learningrate * dD / N\r\n",
    "                # print(\"new\",A,B,C,D,x,y)\r\n",
    "            # else:\r\n",
    "                # if state == 3:\r\n",
    "                #     a = a -temp2a_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                #     b = b -temp2b_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "                # else:\r\n",
    "                #     if state ==4:\r\n",
    "                #         a = a -temp_learningrate * (-2 * x.dot(delta).sum() / N) # on retire un gradient à a\r\n",
    "                #         b = b -temp_learningrate * (-2 * delta.sum() / N) # idem pour b\r\n",
    "                #     else:\r\n",
    "                #         if state == 5:\r\n",
    "                #             temp = x.copy()\r\n",
    "                #             temp2 =x.dot(delta).sum()\r\n",
    "                #             temp3 =x.dot(temp).sum()\r\n",
    "                #             a = a -_learningrate * ((-2 * temp2 / N)*(1 - (-2 * temp3 /N)) ) # on retire un gradient à a\r\n",
    "                #             b = b -_learningrate * ((-2 * delta.sum() / N )*(1 - (2/N)) ) # idem pour b\r\n",
    "                #         else:\r\n",
    "                #             if state == 6:\r\n",
    "                #                 a = a -_learningrate * ((-2 * x.dot(delta).sum() / N)*(1 + (2 * x.dot(x.copy()).sum() /N)) ) # on retire un gradient à a\r\n",
    "                #                 b = b -_learningrate * ((-2 * delta.sum() / N )*(1 + (2/N)) ) # idem pour b\r\n",
    "        # a = a -_learningrate * (-2 * X.dot(delta).sum()) # on retire un gradient à a\r\n",
    "        # b = b -_learningrate * (-2 * delta.sum()) # idem pour b\r\n",
    "        # print(y, (a*X + b),a,X,b)\r\n",
    "        # mse = mean_squared_error(y, (a*x + b))\r\n",
    "        # mse2 = mean_squared_error(y, ( ( (1-A**2*(x-B)**2)/(C**2) )**(1/2) + D ) )\r\n",
    "        # mse2 = mean_squared_error(y, (A*x**2) )\r\n",
    "        # print(mse2)\r\n",
    " \r\n",
    "        # trace = trace.append(pd.DataFrame(data=[[A, dA, B, dB, C, dC, D, dD, my_delta.sum()]], \r\n",
    "        #                                   columns=['A', 'dA', 'B', 'dB', 'C', 'dC', 'D', 'dD', 'my_delta'], \r\n",
    "        #                                   index=['epoch ' + str(i+1)]))\r\n",
    "                                          \r\n",
    "        trace = trace.append(pd.DataFrame(data=[[A, B, C, D, my_delta.sum()]], \r\n",
    "                                          columns=['A', 'B', 'C', 'D', 'my_delta'], \r\n",
    "                                          index=['epoch ' + str(i+1)]))\r\n",
    "        # print(i+1, a, temp_a,temp_a==a, b, temp_b,temp_b==b, mse, temp_mse, temp_mse==mse)\r\n",
    "        # print(i+1)\r\n",
    "        # if temp_A == A and temp_B == B:\r\n",
    "        #     # print(i)\r\n",
    "        #     break\r\n",
    "        # temp_A = A\r\n",
    "        # temp_B = B\r\n",
    "        # temp_mse = mse\r\n",
    "    print(lowest)\r\n",
    "    return A, B, C, D, my_delta.sum(), trace\r\n",
    "# 2320 1.0999999999999988 1.0999999999999988 True  -0.0666666666666641  -0.06666666666666408 False 0.05555555555555553  0.055555555555555504 False\r\n",
    "# 2321 1.0999999999999988 1.0999999999999988 True  -0.0666666666666641  -0.0666666666666641  True  0.05555555555555553  0.05555555555555553  True\r\n",
    "\r\n",
    "# 9999 1.1000000000000514 1.0999999999999488 False -0.06666666666664432 -0.06666666666668944 False 0.055555555555555504 0.05555555555555556  False\r\n",
    "#10000 1.0999999999999488 1.1000000000000514 False -0.06666666666668944 -0.06666666666664432 False 0.05555555555555556  0.055555555555555504 False\r\n",
    "\r\n",
    "# 9999 1.0999999984358646 1.0999999984336053 False -0.06666666311102022 -0.06666666310588418 False 0.055555555555555504 0.05555555555555556  False\r\n",
    "#10000 1.0999999984381206 1.0999999984358646 False -0.06666666311614873 -0.06666666311102022 False 0.05555555555555553  0.055555555555555504 False\r\n",
    "def displayResult(_A, _B, _C, _D, _my_delta, _trace):\r\n",
    "  plt.figure( figsize=(30,5))\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 1)\r\n",
    "  plt.grid(True)\r\n",
    "  plt.title(\"points\")\r\n",
    "  plt.xlabel(\"x, A,C\")\r\n",
    "  plt.ylabel(\"y, B,D\")\r\n",
    "  plt.scatter(Xinit,Yinit)\r\n",
    "  from math import pi, cos, sin\r\n",
    "  t = np.linspace(0, 2*pi, 100)\r\n",
    "  Ell = np.array([_A*np.cos(t) , _B*np.sin(t)])  \r\n",
    "  plt.plot( _C+Ell[0,:] , _D+Ell[1,:], 'g' )     #initial ellipse\r\n",
    "\r\n",
    "  t = np.linspace(0, 2*pi, 100)\r\n",
    "  Ell = np.array([beta[0]*coeff*np.cos(t) , beta[1]*coeff*np.sin(t)])  \r\n",
    "  plt.plot( beta[2]+Ell[0,:] , beta[3]+Ell[1,:], 'r')     #initial ellipse\r\n",
    "#   plt.plot([X[0], X[len(X)-1]], [_a * X[0] + _b, _a * X[len(X)-1] + _b], 'r-', lw=2)\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 2)\r\n",
    "  plt.title(\"A\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['A'])\r\n",
    "  \r\n",
    "#   plt.subplot(1, 9, 3)\r\n",
    "#   plt.title(\"dA\")\r\n",
    "#   plt.plot(_trace['dA'])\r\n",
    " \r\n",
    "  plt.subplot(1, 6, 3)\r\n",
    "  plt.title(\"B\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['B'])\r\n",
    " \r\n",
    "#   plt.subplot(1, 9, 5)\r\n",
    "#   plt.title(\"dB\")\r\n",
    "#   plt.plot(_trace['dB'])\r\n",
    "  \r\n",
    "  plt.subplot(1, 6, 4)\r\n",
    "  plt.title(\"C\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['C'])\r\n",
    "\r\n",
    "#   plt.subplot(1, 9, 7)\r\n",
    "#   plt.title(\"dC\")\r\n",
    "#   plt.plot(_trace['dC'])\r\n",
    "\r\n",
    "  plt.subplot(1, 6, 5)\r\n",
    "  plt.title(\"D\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['D'])\r\n",
    "  \r\n",
    "#   plt.subplot(1, 9, 9)\r\n",
    "#   plt.title(\"dD\")\r\n",
    "#   plt.plot(_trace['dD'])\r\n",
    "\r\n",
    "  plt.subplot(1, 6, 6)\r\n",
    "  plt.title(\"my_delta\")\r\n",
    "  plt.xticks([])\r\n",
    "  # plt.yticks([])\r\n",
    "  plt.plot(_trace['my_delta'])\r\n",
    "  \r\n",
    "  # plt.subplot(1, 5, 5)\r\n",
    "  print (_trace)\r\n",
    "  print(ellipse)\r\n",
    "  plt.show()\r\n",
    "\r\n",
    "# X = [1.0, 2.0, 3.0]\r\n",
    "# y = [1.2, 1.8, 3.4]\r\n",
    "# print(A,B)\r\n",
    "# X = []\r\n",
    "# y = []\r\n",
    "# for i in range(0,1000):\r\n",
    "#     to_x =np.random(0,1)\r\n",
    "#     X.append(to_x)\r\n",
    "#     y.append(A*to_x + B)\r\n",
    "# np.random.seed(1727773457)\r\n",
    "# np.random.seed(72172712)\r\n",
    "\r\n",
    "Ainit,Binit = 1.1, -2/3\r\n",
    "\r\n",
    "num_step = 10000\r\n",
    "num_var = 100\r\n",
    "alpha = [-10, -10, 10, -10]\r\n",
    "beta = [100,100,1000,-1000]\r\n",
    "randi = 1\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "# samples2 = np.random.randn(num_var)\r\n",
    "# X = samples + ellipse[0]\r\n",
    "# Y = ( ( (1-ellipse[2]**2*(X-ellipse[1])**2)/(ellipse[3]**2) )**(1/2) + ellipse[1] ) \r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "# X = ellipse[2]*np.cos(2*samples1) \r\n",
    "# Y = ellipse[3]*np.sin(2*samples1) \r\n",
    "\r\n",
    "# print(X,Y)\r\n",
    "# X1 = samples\r\n",
    "# Y1 = Ainit*samples**2 + Binit *  + np.random.randn(num_var)\r\n",
    "\r\n",
    "def do_show(Aa,Ba,Ca=1,D=1000, learningrate = 0.05):\r\n",
    "    # print(A,B)\r\n",
    "    A, B, C, D, my_delta, trace = gradient_descent(Aa, Ba, Ca, learningrate, _epochs=D)\r\n",
    "    displayResult(A, B, C, D, my_delta, trace )\r\n",
    "\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step) \r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "ellipse = [beta[0] + alpha[0]*abs(np.random.randn()), beta[1] + alpha[1]*abs(np.random.randn()), beta[2] + alpha[2]*(np.random.randn()), beta[3] + alpha[3]*(np.random.randn())]#[centre.x, centre.y, scalar.x, scalar.y]\r\n",
    "samples1 = np.random.randn(num_var)\r\n",
    "Xinit = ellipse[2]+ellipse[0]*np.cos(samples1) + randi * np.random.randn(num_var)\r\n",
    "Yinit = ellipse[3]+ellipse[1]*np.sin(samples1) + randi * np.random.randn(num_var)\r\n",
    "do_show(Xinit.copy(),Yinit.copy(),1,num_step)\r\n",
    "\r\n",
    "# do_show(X.copy(),Y.copy(),2,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),1,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),5,num_step)\r\n",
    "# do_show(X.copy(),Y.copy(),6,num_step)\r\n",
    "# a, b, trace = gradient_descent(X, y, 3, _epochs=100000)\r\n",
    "# displayResult(a, b, trace)\r\n",
    "# a, b, trace = gradient_descent(X, y, 4, _epochs=100000)\r\n",
    "# displayResult(a, b, trace)\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "# print(np.random.RandomState().get_state())\r\n",
    "\r\n",
    "# 0.951333380260973\r\n",
    "# 0.9513333802609729\r\n",
    "# 0.9513349597305195\r\n",
    "# 0.951333380260973\r\n",
    "\r\n",
    "# 285 1.1305812000178983 1.130581200017898 False -0.6648922306427782 -0.6648922306427781 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014590149504239829\r\n",
    "# 286 1.1305812000178983 1.1305812000178983 True -0.6648922306427784 -0.6648922306427782 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014577423674859143\r\n",
    "# 287 1.1305812000178983 1.1305812000178983 True -0.6648922306427785 -0.6648922306427784 False 0.9513333802609731 0.951333380260973 False\r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014564753151219703\r\n",
    "# 288 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427785 False 0.951333380260973 0.9513333802609731 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014552137502179978\r\n",
    "# 289 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427786 True 0.951333380260973 0.951333380260973 True        \r\n",
    "\r\n",
    "# 285 1.1305812000178983 1.130581200017898 False -0.6648922306427782 -0.6648922306427781 False 0.951333380260973 0.951333380260973 True\r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014590149504239829\r\n",
    "# 286 1.1305812000178983 1.1305812000178983 True -0.6648922306427784 -0.6648922306427782 False 0.951333380260973 0.951333380260973 True       \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014577423674859143\r\n",
    "# 287 1.1305812000178983 1.1305812000178983 True -0.6648922306427785 -0.6648922306427784 False 0.9513333802609731 0.951333380260973 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014564753151219703\r\n",
    "# 288 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427785 False 0.951333380260973 0.9513333802609731 False     \r\n",
    "# 0.06 0.13112392898332514 0.00011999999999999999 5.993454529946038e-05 5.999994000009e-05 0.014552137502179978\r\n",
    "# 289 1.1305812000178983 1.1305812000178983 True -0.6648922306427786 -0.6648922306427786 True 0.951333380260973 0.951333380260973 True        \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}